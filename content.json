[{"title":"music note onset detection 论文阅读","date":"2017-06-05T12:37:08.000Z","path":"2017/06/05/music-note-detection-论文阅读/","text":"本文是NOTE ONSET DETECTION IN MUSICAL SIGNALS VIA NEURAL–NETWORK–BASED MULTI–ODF FUSION的阅读笔记。本文主要将ODF：onset detection function与神经网络结合起来，做music note onset的检测。 开始事件检测函数ODF：Onset detection function方法1本文的方法基本上是基于STFT变换，公式$(1)$是基于能量变化做的检测：$$ \\begin{equation}SF(n) = \\sum_{k}hwr(|X_k(n)|-|X_k(n-1)|)\\label{eq:1}\\end{equation}$$其中$X_k(n)$是第n帧的STFT的第k个值，这个值为复数代表幅度与相位信息这里取绝对值，只考虑能量信息，且：$$\\begin{equation}hwr(x) = \\frac{x+|x|}{2}\\label{eq:2}\\end{equation}$$$hwr(x)$即为半波整流函数（half-wave rectifier）仅当$x$为正数时，才有值。这样既可检测能量的突然增加。 方法2相位的变化可用式$(3)$表示：$$\\begin{equation}d\\varphi_k(n) = princarg[\\varphi_k(n)-2\\varphi_k(n-1)+\\varphi_k(n-2)]\\label{eq:3}\\end{equation}$$ 在$WPD(n)$同时考虑了幅度和相位的信息，故：$$\\begin{equation}WPD(n) = \\sum_{k}|hwr(|X_k(n)|-|X_k(n-1)|)\\cdot d\\varphi_k(n)\\label{eq:4}\\end{equation}$$ 方法3一种基于相位谱的更复杂的方法，他们使用相位偏差系数来构建每个STFT帧的相位偏移的二维直方图。 然后可以用一些统计特征来计算结果（例如，峰度）：$$\\begin{equation}PHK(n) = Kurt(h(d\\varphi_k(n)))\\label{eq:5}\\end{equation}$$ 方法4本方法直接在复频谱域进行计算，兼顾了相位与幅度：$$\\begin{equation}CD(n) = \\sum_{k}|\\widehat{S}_k(n)-S_k(n)|\\label{eq:6}\\end{equation}$$其中$|\\widehat{S}_k(n)-S_k(n)|$表示第n个STFT帧第k个频率仓的期望和实际之间的复数差的幅度，其中：$$\\begin{equation}\\widehat{S}_k(n) = |S_k(n-1)|e^{j(2\\varphi_k(n-1)-\\varphi_k(n-2))}\\label{eq:7}\\end{equation}$$以上三种方法皆为传统的ODF，在计算后加上固定的阈值即可检测onset。 混合模型如图所示即为混合模型的整体架构： 模型的输入为：$$\\begin{equation}\\boldsymbol x(n) = [\\boldsymbol v_S(n),\\boldsymbol v_W(n),\\boldsymbol v_P(n),\\boldsymbol v_C(n)]\\label{eq:8}\\end{equation}$$ 其中：$$\\begin{equation}\\boldsymbol v_{S}(n) = [SF(n-2),\\,SF(n-1),\\,SF(n),\\,SF(n+1),\\,SF(n+2),\\,\\overline{SF}(n)]\\label{eq:9}\\end{equation}$$ 其中：$$\\begin{equation}\\overline{SF}(n) = \\sum_{k=n-\\lfloor n_m/2\\rfloor}^{n+\\lfloor n_m/2\\rfloor}SF(k) \\qquad n_m = 21\\label{eq:10}\\end{equation}$$ 模型的输出为$P(onset\\,\\, \\vert n=k)$。由于onset时刻的两边也可以但测到onset，故label的生成采用软条件（soft condition），其中网络的目标输出值为以第n帧为中心的高斯曲线。 在一些简化和舍入之后，将出现在第n帧中的开始的连续目标值设置为:\\begin{equation}\\begin{aligned}t(n) &amp;= 0.75, \\\\t(n\\pm 1) &amp;= 0.55,\\\\t(n\\pm 2) &amp;= 0.35,\\\\t(n\\pm 3) &amp;= 0.25,\\\\t(n\\pm 4) &amp;= 0.25,\\\\\\end{aligned}\\end{equation}曲线如图所示： 神经网络的输出也被当成另一个ODF的输入，这个ODF用峰值检测以及固定的阈值检测onset的时间点。 模型训练及结果在分割训练集与测试集时，文章采用两种方案： 每次训练时，都移除一种乐器，用其余的16中乐器进行训练，被移除的乐器做测试集用，依次重复17次。把单独的音频文件分成10分，每一份都依次做交叉验证集使用，其余9份做训练集用。 以下分别为两种方案的结果： scheme 1 scheme 2 注意：有的乐器的特性与主流的乐器不同，其中一个严重的问题就在于单簧管和萨克斯的实际onset时刻会比记录的ground true晚很多，降低了第一种方案训练的准确性。各乐器的最佳偏移量如下图所示，评价标准为F-measure。 文章通过试验，找到了较好的神经网络输出的阈值以及特征值的子集，最终的结果为：","tags":[{"name":"paper reading","slug":"paper-reading","permalink":"http://yoursite.com/tags/paper-reading/"},{"name":"neural network","slug":"neural-network","permalink":"http://yoursite.com/tags/neural-network/"},{"name":"music note detection","slug":"music-note-detection","permalink":"http://yoursite.com/tags/music-note-detection/"},{"name":"ODF","slug":"ODF","permalink":"http://yoursite.com/tags/ODF/"}]},{"title":"tensorflow.train.learning_rate_decay()","date":"2017-03-20T14:01:43.000Z","path":"2017/03/20/tensorflow-train-learning-rate-decay/","text":"学习udacity/3_regularization.ipynb时，发现tf.train.exponential_decay()函数，阅读源码，大概知道意思： 源码截取如下： 1234567891011121314151617def exponential_decay(learning_rate, global_step, decay_steps, decay_rate, staircase=False, name=None): if global_step is None: raise ValueError(\"global_step is required for exponential_decay.\") with ops.name_scope(name, \"ExponentialDecay\", [learning_rate, global_step, decay_steps, decay_rate]) as name: learning_rate = ops.convert_to_tensor(learning_rate, name=\"learning_rate\") dtype = learning_rate.dtype global_step = math_ops.cast(global_step, dtype) decay_steps = math_ops.cast(decay_steps, dtype) decay_rate = math_ops.cast(decay_rate, dtype) p = global_step / decay_steps if staircase: p = math_ops.floor(p) return math_ops.multiply(learning_rate, math_ops.pow(decay_rate, p), name=name) 看注释及代码可知 $$decayed\\ learning\\ rate = learning\\ rate \\times decay\\ rate ^ {global\\ step / decay\\ steps}$$ 即每隔global_step步进行一次递减，递减的方式为上一个learing_rate乘上decay_rate。由此可见，使用该方法下降learning_rate可使其迅速减小。 同时在Problem 4中，我使用了5层神经网络，用了L2正则以及dropout。我发现增加网络的层数并没有显著提高正确率，增加batch_size却可以提高正确率至96.4%。","tags":[{"name":"tensorflow","slug":"tensorflow","permalink":"http://yoursite.com/tags/tensorflow/"},{"name":"Machine learning","slug":"Machine-learning","permalink":"http://yoursite.com/tags/Machine-learning/"}]},{"title":"Bug日记","date":"2017-03-13T10:24:50.000Z","path":"2017/03/13/Bug日记/","text":"得到了一个iPod并希望在上面调试程序，但是总是显示App installation failed - There was an error communicating with your device.在官网上并没有搜索在解决方案。最后将iPod强行恢复出厂设置，bug fix！","tags":[{"name":"iOS","slug":"iOS","permalink":"http://yoursite.com/tags/iOS/"}]},{"title":"制作POP翻页动画","date":"2016-07-26T12:16:59.000Z","path":"2016/07/26/制作POP翻页动画/","text":"我利用了POP库制作了简单的广告展示的翻页效果： yanshi 源码在这里 大致思路首先简单分析一下动画效果：我们使用了旋转以及平移两个动画效果，通过动图可以看到：通过手指拖动(pan)，图片在平移的同时绕着自身中轴线旋转。那么怎么让图片绕着中轴线旋转呢？可以使用POPBasicAnimation中的kPOPLayerRotaionY制作旋转动画，中轴线的位置为锚点的位置，可以通过anchorPoint、anchorPointZ以及position共同确定锚点的三维坐标。至于anchorPoint与position如何共同确定绝对坐标可以参照博客，简单来说：锚点是标志一个layer位置的点，anchorPoint是锚点在该layer的相对坐标，而positon是锚点在superView中的绝对坐标。沿x轴的平移效果可以通过POPBasicAnimaition的kPOPLayerPositionX来实现。旋转的角度以及平移的距离都与手指拖动(pan)的距离有关。 源码分析首先初始化五张图片，设置不同的位置，以及不同的旋转角度。在画面中只显示3张，但为了左右拖动时有图片补位，这里初始化5张。 12345678910111213141516171819202122- (void)initPictures &#123; for (int i = 0; i&lt;5; i++) &#123; UIImageView *imgView = [[UIImageView alloc]initWithImage:[UIImage imageNamed:[pictureNames objectAtIndex:i]]]; imgView.frame = CGRectMake(0, 0, 319, 187); imgView.layer.anchorPoint = CGPointMake(0.5, 0.5); imgView.layer.anchorPointZ = 100.0f; imgView.layer.position = CGPointMake(-62+i*287, 374/4+30); imgView.layer.transform = [self setTransform3D]; imgView.userInteractionEnabled = YES; POPBasicAnimation *initRotationAnimation = [POPBasicAnimation animationWithPropertyNamed:kPOPLayerRotationY]; initRotationAnimation.duration = 0; initRotationAnimation.toValue = @((2-i)*M_PI/6); [imgView.layer pop_addAnimation:initRotationAnimation forKey:@&quot;initRotation&quot;]; [self showImageAndReflection:imgView]; [imgViews addObject:imgView]; [self.view addSubview:imgView]; &#125;&#125; 这里通过showImageAndReflection函数来设置图片的倒影： 12345678910111213141516171819202122232425262728293031- (void)showImageAndReflection:(UIImageView *)view &#123; // 制作reflection CALayer *layer = view.layer; CALayer *reflectLayer = [CALayer layer]; reflectLayer.contents = layer.contents; reflectLayer.bounds = layer.bounds; reflectLayer.position = CGPointMake(layer.bounds.size.width/2, layer.bounds.size.height*1.5); reflectLayer.transform = CATransform3DMakeRotation(M_PI, 1, 0, 0); // 给该reflection加个半透明的layer CALayer *blackLayer = [CALayer layer]; blackLayer.backgroundColor = [UIColor whiteColor].CGColor; blackLayer.bounds = reflectLayer.bounds; blackLayer.position = CGPointMake(blackLayer.bounds.size.width/2, blackLayer.bounds.size.height/2); blackLayer.opacity = 0.6; [reflectLayer addSublayer:blackLayer]; // 给该reflection加个mask CAGradientLayer *mask = [CAGradientLayer layer]; mask.bounds = reflectLayer.bounds; mask.position = CGPointMake(mask.bounds.size.width/2, mask.bounds.size.height/2); mask.colors = [NSArray arrayWithObjects: (__bridge id)[UIColor clearColor].CGColor, (__bridge id)[UIColor whiteColor].CGColor, nil]; mask.startPoint = CGPointMake(0.5, 0.65); mask.endPoint = CGPointMake(0.5, 1); reflectLayer.mask = mask; // 作为layer的sublayer [layer addSublayer:reflectLayer];&#125; 代码参考这里 随后设置拖动手势UIPanGestureRecognizer并绑定panAllHandle函数，将手势绑定在当前view上。 12UIPanGestureRecognizer *panGesture = [[UIPanGestureRecognizer alloc]initWithTarget:self action:@selector(panAllHandle:)];[self.view addGestureRecognizer:panGesture]; 处理函数panAllHandle为： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138- (void)panAllHandle:(UIPanGestureRecognizer *)recognizer &#123; CGPoint location = [recognizer locationInView:self.view]; if (recognizer.state == UIGestureRecognizerStateBegan) &#123; num = location.x; &#125; NSMutableArray *rotationAnimations = [[NSMutableArray alloc]init]; NSMutableArray *moveAnimations = [[NSMutableArray alloc]init]; NSMutableArray *rotationEndAnimations = [[NSMutableArray alloc]init]; NSMutableArray *moveEndAnimations = [[NSMutableArray alloc]init]; for (int i = 0; i&lt;5; i++) &#123; POPBasicAnimation *rotationAnimation = [POPBasicAnimation animationWithPropertyNamed:kPOPLayerRotationY]; rotationAnimation.duration = 0.01; POPBasicAnimation *moveAnimation = [POPBasicAnimation animationWithPropertyNamed:kPOPLayerPositionX]; moveAnimation.duration = 0.01; [rotationAnimations addObject:rotationAnimation]; [moveAnimations addObject:moveAnimation]; &#125; if (YES) &#123; CGFloat percent = M_PI / (6*287); POPBasicAnimation *rotationAnimation = [POPBasicAnimation animationWithPropertyNamed:kPOPLayerRotationY]; rotationAnimation.duration = 0.01; NSLog(@&quot;%f, %lu&quot;,location.x, (unsigned long)num); POPBasicAnimation *moveAnimation = [POPBasicAnimation animationWithPropertyNamed:kPOPLayerPositionX]; moveAnimation.duration = 0.01; if ((location.x-num)&gt;350) &#123; for (int i = 0; i&lt;4; i++) &#123; POPBasicAnimation *rotation = [rotationAnimations objectAtIndex:i]; rotation.toValue = @(-(350)*percent+(2-i)*M_PI/6); POPBasicAnimation *move = [moveAnimations objectAtIndex:i]; move.toValue = @(350-62+i*287); UIImageView *imgView = [imgViews objectAtIndex:i]; [imgView.layer pop_addAnimation:rotation forKey:@&quot;rotation&quot;]; [imgView.layer pop_addAnimation:move forKey:@&quot;move&quot;]; &#125; &#125; else if ((location.x-num)&lt;-350) &#123; for (int i = 1; i&lt;5; i++) &#123; POPBasicAnimation *rotation = [rotationAnimations objectAtIndex:i]; rotation.toValue = @(-(-350)*percent+(2-i)*M_PI/6); POPBasicAnimation *move = [moveAnimations objectAtIndex:i]; move.toValue = @(-350-62+i*287); UIImageView *imgView = [imgViews objectAtIndex:i]; [imgView.layer pop_addAnimation:rotation forKey:@&quot;rotation&quot;]; [imgView.layer pop_addAnimation:move forKey:@&quot;move&quot;]; &#125; &#125; else if ((location.x - num)&gt;0) &#123; for (int i = 0; i&lt;4; i++) &#123; POPBasicAnimation *rotation = [rotationAnimations objectAtIndex:i]; rotation.toValue = @(-(location.x-num)*percent+(2-i)*M_PI/6); POPBasicAnimation *move = [moveAnimations objectAtIndex:i]; move.toValue = @(location.x-num-62+i*287); UIImageView *imgView = [imgViews objectAtIndex:i]; [imgView.layer pop_addAnimation:rotation forKey:@&quot;rotation&quot;]; [imgView.layer pop_addAnimation:move forKey:@&quot;move&quot;]; &#125; &#125; else if ((location.x - num)&lt;=0) &#123; for (int i = 1; i&lt;5; i++) &#123; POPBasicAnimation *rotation = [rotationAnimations objectAtIndex:i]; rotation.toValue = @(-(location.x-num)*percent+(2-i)*M_PI/6); POPBasicAnimation *move = [moveAnimations objectAtIndex:i]; move.toValue = @(location.x-num-62+i*287); UIImageView *imgView = [imgViews objectAtIndex:i]; [imgView.layer pop_addAnimation:rotation forKey:@&quot;rotation&quot;]; [imgView.layer pop_addAnimation:move forKey:@&quot;move&quot;]; &#125; &#125; if (recognizer.state == UIGestureRecognizerStateEnded || recognizer.state == UIGestureRecognizerStateCancelled) &#123; for (int i = 0; i&lt;5; i++) &#123; POPSpringAnimation *recoverAnimation = [POPSpringAnimation animationWithPropertyNamed:kPOPLayerRotationY]; recoverAnimation.springBounciness = 18.0f; recoverAnimation.dynamicsMass = 2.0f; recoverAnimation.dynamicsTension = 200; POPSpringAnimation *recover = [POPSpringAnimation animationWithPropertyNamed:kPOPLayerPositionX]; recover.springBounciness = 18.0f; recover.dynamicsMass = 2.0f; recover.dynamicsTension = 200; [rotationEndAnimations addObject:recoverAnimation]; [moveEndAnimations addObject:recover]; &#125; POPSpringAnimation *recoverAnimation = [POPSpringAnimation animationWithPropertyNamed:kPOPLayerRotationY]; recoverAnimation.springBounciness = 18.0f; recoverAnimation.dynamicsMass = 2.0f; recoverAnimation.dynamicsTension = 200; // initialLocation = -img.frame.origin.x-img.frame.size.width/2; if ((location.x-num)&gt;50) &#123; for (int i = 0; i&lt;4; i++) &#123; POPBasicAnimation *rotation = [rotationEndAnimations objectAtIndex:i]; rotation.toValue = @(-(287)*percent+(2-i)*M_PI/6); POPBasicAnimation *move = [moveEndAnimations objectAtIndex:i]; move.toValue = @(287-62+i*287); UIImageView *imgView = [imgViews objectAtIndex:i]; [imgView.layer pop_addAnimation:rotation forKey:@&quot;rotation&quot;]; [imgView.layer pop_addAnimation:move forKey:@&quot;move&quot;]; &#125; [self moveRight]; &#125; else if ((location.x-num)&lt;-50) &#123; for (int i = 1; i&lt;5; i++) &#123; POPBasicAnimation *rotation = [rotationEndAnimations objectAtIndex:i]; rotation.toValue = @(-(-287)*percent+(2-i)*M_PI/6); POPBasicAnimation *move = [moveEndAnimations objectAtIndex:i]; move.toValue = @(-287-62+i*287); UIImageView *imgView = [imgViews objectAtIndex:i]; [imgView.layer pop_addAnimation:rotation forKey:@&quot;rotation&quot;]; [imgView.layer pop_addAnimation:move forKey:@&quot;move&quot;]; &#125; [self moveLeft]; &#125; else &#123; for (int i = 0; i&lt;5; i++) &#123; POPBasicAnimation *rotation = [rotationEndAnimations objectAtIndex:i]; rotation.toValue = @((2-i)*M_PI/6); POPBasicAnimation *move = [moveEndAnimations objectAtIndex:i]; move.toValue = @(-62+i*287); UIImageView *imgView = [imgViews objectAtIndex:i]; [imgView.layer pop_addAnimation:rotation forKey:@&quot;rotation&quot;]; [imgView.layer pop_addAnimation:move forKey:@&quot;move&quot;]; &#125; &#125; &#125; &#125;&#125; 由于重复的动画过多，这里只解释其中两种：初始化旋转以及平移函数,duration代表持续时间。 1234POPBasicAnimation *rotationAnimation = [POPBasicAnimation animationWithPropertyNamed:kPOPLayerRotationY];rotationAnimation.duration = 0.01;POPBasicAnimation *moveAnimation = [POPBasicAnimation animationWithPropertyNamed:kPOPLayerPositionX];moveAnimation.duration = 0.01; 通过position.x与num的差值，获得拖动的x轴方向距离。根据该距离设置好旋转动画旋转的角度，以及平移动画平移的距离。 12rotation.toValue = @(-(location.x-num)*percent+(2-i)*M_PI/6);move.toValue = @(location.x-num-62+i*287); 最后将动画绑定在图片上： 12[imgView.layer pop_addAnimation:rotation forKey:@&quot;rotation&quot;];[imgView.layer pop_addAnimation:move forKey:@&quot;move&quot;]; 这样一个图片的动画就完成了。最后推广在到所有图片上，并为拖动距离添加一定的阈值即可完成动画。","tags":[{"name":"iOS","slug":"iOS","permalink":"http://yoursite.com/tags/iOS/"},{"name":"POP","slug":"POP","permalink":"http://yoursite.com/tags/POP/"}]},{"title":"将cocos工程加入到iOS工程中","date":"2016-07-05T12:43:50.000Z","path":"2016/07/05/将cocos工程加入到iOS工程中/","text":"本文讨论的是这样一种情况：一个iOS应用中有一个游戏，iOS程序与游戏是单独开发，并且游戏是用cocos2dx引擎。这样在开发结束时，我们需要将这两个工程合到一起，换句话说就是讲cocos游戏嵌入到iOS应用中。 大致思路我的解决方案的大致思路是这样的：将cocos引擎添加到iOS工程中，在合适的位置进入cocos游戏界面。 步骤向iOS工程中加入文件首先按cocos官网上的方法生成cocos工程，向iOS工程中添加cocos工程中的Classes/、proj.ios_mac/ios/、resources复制到iOS工程的文件夹中。向工程中添加文件：Classes/、ios/、cocos2d/build/cocos2d_libs.xcodeproj。 更改工程设置1.向工程中添加如图所示的库 Markdown 2.向 Building Setting&gt;Header Search Paths 中添加项 Markdown 3.在 Building Setting 中更改工程设置 Markdown Markdown 4.删除原工程的 main.m、AppDelegate.h/m 文件 更改工程入口具体设置参照博客，大致的思路就是修改cocos工程的入口AppController文件，在需要嵌入游戏的位置初始化RootViewController即可。 合并后的工程在这里.","tags":[{"name":"iOS","slug":"iOS","permalink":"http://yoursite.com/tags/iOS/"},{"name":"cocos2dx","slug":"cocos2dx","permalink":"http://yoursite.com/tags/cocos2dx/"}]},{"title":"开始我的hexo博客","date":"2016-03-23T08:05:00.000Z","path":"2016/03/23/开始我的hexo博客/","text":"很早就有写博客的想法了，今天终于用hexo搭起了自己的博客，记录一下过程： 安装hexo使用nvm安装Node.js 1$ wget -qO- https://raw.github.com/creationix/nvm/master/install.sh | sh 安装完成后，重启终端并执行下列命令即可安装 Node.js 1$ nvm install 4 安装Hexo 1$ npm install -g hexo-cli 安装部署到git的插件 1$ npm install hexo-deployer-git --save 配置Github创建名为yourname.github.io的repository开启gh-pages功能 配置hexo按照hexo官网上的配置步骤进行注意配置_config.yml中的language选项时，需要看主题中language/里的语言文件 使用Next主题123$ cd theme/$ git clone https://github.com/iissnan/hexo-theme-next.git$ vim ../_config.yml 将theme:改为theme: hexo-theme-next 部署到Github修改配置完成后执行下列语句，生成代码 1$ hexo g 执行下列语句，部署到Github上 1$ hexo d","tags":[{"name":"hexo","slug":"hexo","permalink":"http://yoursite.com/tags/hexo/"}]}]